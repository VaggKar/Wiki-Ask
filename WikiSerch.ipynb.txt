{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3977a71f-e5b4-4294-9f54-b2fab03ecd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data (run only the first time)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stop words and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d87343a7-c8e5-4e5d-b4e8-f4a90cf51105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article saved to 'wikipedia_article.json'.\n"
     ]
    }
   ],
   "source": [
    "def scrape_wikipedia_article(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract all paragraphs\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = \"\\n\".join([para.get_text() for para in paragraphs if para.get_text()])\n",
    "    \n",
    "    # Save article to JSON format\n",
    "    article_data = {\n",
    "        'url': url,\n",
    "        'content': text\n",
    "    }\n",
    "    \n",
    "    with open(\"wikipedia_article.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(article_data, f, indent=4)\n",
    "    print(\"Article saved to 'wikipedia_article.json'.\")\n",
    "\n",
    "# Scrape Wikipedia article\n",
    "url = \"https://en.wikipedia.org/wiki/Greece\"  # Replace with any article URL\n",
    "scrape_wikipedia_article(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae833d58-42ca-4dc7-b16e-b4dce222f7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–',\n",
       " 'europ',\n",
       " 'light',\n",
       " 'green',\n",
       " 'dark',\n",
       " 'grey–',\n",
       " 'european',\n",
       " 'union',\n",
       " 'light',\n",
       " 'green',\n",
       " 'greecea',\n",
       " 'offici',\n",
       " 'hellen',\n",
       " 'republicb',\n",
       " 'countri',\n",
       " 'southeast',\n",
       " 'europ',\n",
       " 'locat',\n",
       " 'southern',\n",
       " 'tip']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load article content\n",
    "with open(\"wikipedia_article.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    article_data = json.load(f)\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    return [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "# Preprocess article content\n",
    "processed_content = preprocess_text(article_data[\"content\"])\n",
    "processed_content[:20]  # Display first 20 tokens as a sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34b66b69-5b03-43ad-8f2e-3bf0fac3dfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index saved to 'inverted_index.json'\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_inverted_index(tokens):\n",
    "    index = defaultdict(list)\n",
    "    for position, token in enumerate(tokens):\n",
    "        index[token].append(position)\n",
    "    return index\n",
    "\n",
    "# Create and save the inverted index\n",
    "inverted_index = create_inverted_index(processed_content)\n",
    "with open(\"inverted_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(inverted_index, f, indent=4)\n",
    "print(\"Inverted index saved to 'inverted_index.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "663c7085-6f8a-4495-9223-3445cc66af2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents matching query: {8520, 6597, 6455}\n"
     ]
    }
   ],
   "source": [
    "# Load the inverted index\n",
    "with open(\"inverted_index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    inverted_index = json.load(f)\n",
    "\n",
    "# Boolean retrieval function\n",
    "def boolean_retrieval(query_terms, index, operation=\"AND\"):\n",
    "    result_docs = set(index.get(query_terms[0], []))\n",
    "    for term in query_terms[1:]:\n",
    "        term_docs = set(index.get(term, []))\n",
    "        if operation == \"AND\":\n",
    "            result_docs.intersection_update(term_docs)\n",
    "        elif operation == \"OR\":\n",
    "            result_docs.update(term_docs)\n",
    "        elif operation == \"NOT\":\n",
    "            result_docs.difference_update(term_docs)\n",
    "    return result_docs\n",
    "\n",
    "# Example query terms\n",
    "query_terms = [\"blue\", \"histori\"]\n",
    "result_docs = boolean_retrieval(query_terms, inverted_index, \"OR\")\n",
    "print(\"Documents matching query:\", result_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "402d038c-82d5-459b-ac09-176fc476c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents: [6455, 8520, 6597]\n"
     ]
    }
   ],
   "source": [
    "# Calculate TF-IDF\n",
    "def calculate_tf_idf(term, doc_id, index, total_docs):\n",
    "    tf = len(index[term]) if doc_id in index[term] else 0\n",
    "    idf = math.log(total_docs / len(index[term])) if term in index else 0\n",
    "    return tf * idf\n",
    "\n",
    "# Rank documents based on TF-IDF\n",
    "def rank_documents(query_terms, result_docs, index, total_docs):\n",
    "    doc_scores = {doc_id: 0 for doc_id in result_docs}\n",
    "    for term in query_terms:\n",
    "        for doc_id in result_docs:\n",
    "            doc_scores[doc_id] += calculate_tf_idf(term, doc_id, index, total_docs)\n",
    "    ranked_docs = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    return [doc[0] for doc in ranked_docs]\n",
    "\n",
    "# Ranking example\n",
    "total_docs = 1  # Update if you have more documents\n",
    "ranked_docs = rank_documents(query_terms, result_docs, inverted_index, total_docs)\n",
    "print(\"Ranked Documents:\", ranked_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f8777c8-33af-409e-87d2-273b3934194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query (type 'exit' to quit):  blue OR histori\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents matching query 'blue OR histori': [6455, 8520, 6597]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_query(query):\n",
    "    query = query.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(query)\n",
    "    return [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "def search(query, index, total_docs):\n",
    "    query_terms = preprocess_query(query)\n",
    "    # Determine operation from query\n",
    "    if \" AND \" in query:\n",
    "        operation = \"AND\"\n",
    "    elif \" OR \" in query:\n",
    "        operation = \"OR\"\n",
    "    elif \" NOT \" in query:\n",
    "        operation = \"NOT\"\n",
    "    else:\n",
    "        operation = \"AND\"\n",
    "    \n",
    "    result_docs = boolean_retrieval(query_terms, index, operation)\n",
    "    if result_docs:\n",
    "        return rank_documents(query_terms, result_docs, index, total_docs)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Interactive query\n",
    "query = input(\"Enter your search query (type 'exit' to quit): \")\n",
    "if query.lower() != \"exit\":\n",
    "    results = search(query, inverted_index, total_docs)\n",
    "    print(f\"Documents matching query '{query}': {results}\")\n",
    "else:\n",
    "    print(\"Exiting the search.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0e5e0-178d-4834-91af-c89808f53e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
