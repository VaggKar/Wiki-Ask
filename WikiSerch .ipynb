{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8f741fff-0a09-4c4b-9517-c70b70dda355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "#!pip install requests beautifulsoup4 nltk\n",
    "\n",
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download NLTK stopwords and punkt tokenizer\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download(\"punkt\")\n",
    "\n",
    "# Initialize stop words and stemmer\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "4f4511af-9c43-443f-b7db-aad6ec015fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Wikipedia page titles separated by commas (e.g., 'Artificial Intelligence, Machine Learning, Data Science'):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " New Jersey , Breaking Bad ,The Sopranos\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user for Wikipedia page titles\n",
    "print(\"Enter Wikipedia page titles separated by commas (e.g., 'Artificial Intelligence, Machine Learning, Data Science'):\")\n",
    "page_titles_input = input().split(',')\n",
    "\n",
    "# Clean up and prepare page titles list\n",
    "page_titles = [title.strip() for title in page_titles_input]\n",
    "\n",
    "# Function to Scrape Wikipedia Pages\n",
    "def get_wikipedia_page_content(page_title):\n",
    "    url = f\"https://en.wikipedia.org/wiki/{page_title.replace(' ', '_')}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    \n",
    "    # Extract text content from paragraphs\n",
    "    content = \" \".join([para.text for para in paragraphs if para.text])\n",
    "    return {\"title\": page_title, \"content\": content}\n",
    "\n",
    "# Fetch documents for each title provided by the user\n",
    "documents = [get_wikipedia_page_content(title) for title in page_titles]\n",
    "\n",
    "# Save raw documents to JSON file\n",
    "with open(\"wikipedia_data.json\", \"w\") as f:\n",
    "    json.dump(documents, f)\n",
    "\n",
    "# Display fetched documents to verify content\n",
    "#documents[:1]  # Show first document as an example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "646394b7-1bd4-4edf-8085-f224a7f0fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    # Tokenize, remove stop words, and apply stemming\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to each document and save to processed_data.json\n",
    "processed_documents = [{\"title\": doc[\"title\"], \"content\": preprocess_text(doc[\"content\"])} for doc in documents]\n",
    "\n",
    "with open(\"processed_data.json\", \"w\") as f:\n",
    "    json.dump(processed_documents, f)\n",
    "\n",
    "# Display processed documents to verify\n",
    "#processed_documents[:1]  # Show first processed document as an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "4edece46-7b62-45af-b986-dc95654521d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an inverted index\n",
    "inverted_index = defaultdict(dict)\n",
    "\n",
    "# Populate the inverted index with term frequencies\n",
    "for doc_id, doc in enumerate(processed_documents):\n",
    "    for term in doc[\"content\"]:\n",
    "        if doc_id in inverted_index[term]:\n",
    "            inverted_index[term][doc_id] += 1\n",
    "        else:\n",
    "            inverted_index[term][doc_id] = 1\n",
    "\n",
    "# Save the inverted index to a JSON file (optional)\n",
    "with open(\"inverted_index.json\", \"w\") as f:\n",
    "    json.dump(inverted_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a04998d5-0f3b-4513-b0ea-42c474f53515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the query with Boolean operators\n",
    "def parse_query(query):\n",
    "    terms = query.lower().split()\n",
    "    tokens = []\n",
    "    operators = {\"and\", \"or\", \"not\"}\n",
    "    \n",
    "    for term in terms:\n",
    "        if term in operators:\n",
    "            tokens.append(term)\n",
    "        else:\n",
    "            processed_term = stemmer.stem(term) if term not in stop_words else \"\"\n",
    "            if processed_term:\n",
    "                tokens.append(processed_term)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Boolean retrieval function\n",
    "def boolean_retrieval(query_tokens):\n",
    "    result_set = set(range(len(processed_documents)))  # Start with all documents\n",
    "    current_operation = \"and\"\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token in {\"and\", \"or\", \"not\"}:\n",
    "            current_operation = token\n",
    "        else:\n",
    "            matching_docs = set(inverted_index.get(token, {}).keys())\n",
    "            if current_operation == \"and\":\n",
    "                result_set &= matching_docs\n",
    "            elif current_operation == \"or\":\n",
    "                result_set |= matching_docs\n",
    "            elif current_operation == \"not\":\n",
    "                result_set -= matching_docs\n",
    "    \n",
    "    return list(result_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "bcb5b00d-e889-4b4c-a48b-f1c74191d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF weight for a term in a document\n",
    "def compute_tf_idf(term, doc_id):\n",
    "    term_frequency = inverted_index[term].get(doc_id, 0)\n",
    "    if term_frequency == 0:\n",
    "        return 0\n",
    "    document_frequency = len(inverted_index[term])\n",
    "    inverse_document_frequency = math.log(len(processed_documents) / (1 + document_frequency))\n",
    "    return term_frequency * inverse_document_frequency\n",
    "\n",
    "# Rank results by TF-IDF scores\n",
    "def rank_results_tf_idf(query_tokens, result_docs):\n",
    "    doc_scores = {}\n",
    "    for doc_id in result_docs:\n",
    "        score = 0\n",
    "        for term in query_tokens:\n",
    "            if term not in {\"and\", \"or\", \"not\"}:\n",
    "                score += compute_tf_idf(term, doc_id)\n",
    "        doc_scores[doc_id] = score\n",
    "    return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e1bf2f14-5e1d-4971-b4bb-fa43ccab3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert document to TF-IDF vector\n",
    "def document_to_vector(doc_id):\n",
    "    doc_vector = {}\n",
    "    for term in processed_documents[doc_id][\"content\"]:\n",
    "        doc_vector[term] = compute_tf_idf(term, doc_id)\n",
    "    return doc_vector\n",
    "\n",
    "# Convert query to TF-IDF vector\n",
    "def query_to_vector(query_tokens):\n",
    "    query_vector = {}\n",
    "    for term in query_tokens:\n",
    "        if term not in {\"and\", \"or\", \"not\"}:\n",
    "            term_frequency = query_tokens.count(term)\n",
    "            document_frequency = len(inverted_index.get(term, []))\n",
    "            inverse_document_frequency = math.log(len(processed_documents) / (1 + document_frequency))\n",
    "            query_vector[term] = (term_frequency / len(query_tokens)) * inverse_document_frequency\n",
    "    return query_vector\n",
    "\n",
    "# Compute cosine similarity between two vectors\n",
    "def cosine_similarity(query_vector, doc_vector):\n",
    "    dot_product = sum(query_vector[term] * doc_vector.get(term, 0) for term in query_vector)\n",
    "    query_norm = math.sqrt(sum(val ** 2 for val in query_vector.values()))\n",
    "    doc_norm = math.sqrt(sum(val ** 2 for val in doc_vector.values()))\n",
    "    if query_norm == 0 or doc_norm == 0:\n",
    "        return 0\n",
    "    return dot_product / (query_norm * doc_norm)\n",
    "\n",
    "# VSM retrieval function\n",
    "def vector_space_model(query_tokens):\n",
    "    query_vector = query_to_vector(query_tokens)\n",
    "    doc_scores = {}\n",
    "    for doc_id in range(len(processed_documents)):\n",
    "        doc_vector = document_to_vector(doc_id)\n",
    "        similarity = cosine_similarity(query_vector, doc_vector)\n",
    "        if similarity > 0:\n",
    "            doc_scores[doc_id] = similarity\n",
    "    return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "075d0c2f-cce7-4fcd-b6c8-3431ff13e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main search function with algorithm selection\n",
    "def search_documents(query, algorithm=\"tf-idf\"):\n",
    "    query_tokens = parse_query(query)\n",
    "    \n",
    "    if algorithm == \"boolean\":\n",
    "        results = boolean_retrieval(query_tokens)\n",
    "        print(\"Boolean Retrieval Results:\")\n",
    "        for doc_id in results:\n",
    "            print(f\"Document: {documents[doc_id]['title']}\")\n",
    "        return results\n",
    "    elif algorithm == \"tf-idf\":\n",
    "        result_docs = boolean_retrieval(query_tokens)\n",
    "        ranked_results = rank_results_tf_idf(query_tokens, result_docs)\n",
    "        print(\"\\nTF-IDF Ranked Results (Document ID and Score):\")\n",
    "        for doc_id, score in ranked_results:\n",
    "            print(f\"Document: {documents[doc_id]['title']} - Score: {score:.4f}\")\n",
    "        return ranked_results\n",
    "    elif algorithm == \"vsm\":\n",
    "        ranked_results = vector_space_model(query_tokens)\n",
    "        print(\"\\nVector Space Model Ranked Results (Document ID and Cosine Similarity):\")\n",
    "        for doc_id, similarity in ranked_results:\n",
    "            print(f\"Document: {documents[doc_id]['title']} - Cosine Similarity: {similarity:.4f}\")\n",
    "        return ranked_results\n",
    "    else:\n",
    "        print(\"Unsupported algorithm selected.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ff1ce474-4f8f-46b4-ba7d-9762909e3e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " New Mexico OR New Jersey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose retrieval algorithm:\n",
      "1. Boolean Retrieval\n",
      "2. TF-IDF Ranking\n",
      "3. Vector Space Model\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1, 2, or 3):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Ranked Results (Document ID and Score):\n",
      "Document: Breaking Bad - Score: -5.7536\n",
      "Document: The Sopranos - Score: -30.4943\n",
      "Document: New Jersey - Score: -193.3224\n"
     ]
    }
   ],
   "source": [
    "# Prompt user for query and algorithm selection\n",
    "print(\"Enter your search query:\")\n",
    "query = input()\n",
    "\n",
    "print(\"Choose retrieval algorithm:\\n1. Boolean Retrieval\\n2. TF-IDF Ranking\\n3. Vector Space Model\")\n",
    "choice = input(\"Enter choice (1, 2, or 3): \")\n",
    "\n",
    "if choice == \"1\":\n",
    "    search_documents(query, algorithm=\"boolean\")\n",
    "elif choice == \"2\":\n",
    "    search_documents(query, algorithm=\"tf-idf\")\n",
    "elif choice == \"3\":\n",
    "    search_documents(query, algorithm=\"vsm\")\n",
    "else:\n",
    "    print(\"Invalid choice. Please enter 1, 2, or 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6bafc-de36-49a7-ba01-8a94c8065ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
